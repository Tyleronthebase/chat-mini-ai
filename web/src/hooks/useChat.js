import { useCallback, useRef, useState } from "react";

// â”€â”€â”€ Mock streaming for testing frontend rendering â”€â”€â”€
const MOCK_REPLY = `ä½ å¥½ï¼æˆ‘æ˜¯ **Mock æ¨¡å¼**ï¼Œç”¨äºæµ‹è¯•å‰ç«¯æ¸²æŸ“æ•ˆæœã€‚

è¿™æ˜¯ä¸€æ®µç¤ºä¾‹å›å¤ï¼ŒåŒ…å«å„ç§ Markdown å…ƒç´ ï¼š

## ä»£ç å—
\`\`\`javascript
function hello() {
  console.log("Hello, World!");
}
\`\`\`

## åˆ—è¡¨
1. ç¬¬ä¸€é¡¹ âœ…
2. ç¬¬äºŒé¡¹ ğŸš€
3. ç¬¬ä¸‰é¡¹ ğŸ’¡

> è¿™æ˜¯ä¸€æ®µå¼•ç”¨æ–‡å­—ï¼Œç”¨äºæµ‹è¯•å¼•ç”¨å—æ¸²æŸ“ã€‚

æµå¼è¾“å‡ºæµ‹è¯•å®Œæˆï¼æ¯ä¸ª chunk ä¼šé€æ­¥æ˜¾ç¤ºã€‚`;

async function* mockStream() {
    const chunkSize = 8;
    for (let i = 0; i < MOCK_REPLY.length; i += chunkSize) {
        yield MOCK_REPLY.slice(i, i + chunkSize);
        await new Promise((r) => setTimeout(r, 50));
    }
}

// â”€â”€â”€ OpenAI-compatible streaming (direct from browser) â”€â”€â”€
async function* openaiStream(messages, { apiBase, apiKey, model, systemPrompt, signal }) {
    const base = (apiBase || "").replace(/\/+$/, "");
    const endpoint = `${base}/v1/chat/completions`;

    const openaiMessages = [];

    if (systemPrompt) {
        openaiMessages.push({ role: "system", content: systemPrompt });
    }

    for (const msg of messages) {
        if (msg && typeof msg.content === "string") {
            openaiMessages.push({
                role: msg.role === "model" ? "assistant" : msg.role,
                content: msg.content
            });
        }
    }

    const response = await fetch(endpoint, {
        method: "POST",
        headers: {
            "Content-Type": "application/json",
            Authorization: `Bearer ${apiKey || ""}`
        },
        body: JSON.stringify({
            model: model || "gemini-2.5-flash",
            messages: openaiMessages,
            stream: true,
            temperature: 0.7
        }),
        signal
    });

    if (!response.ok) {
        const detail = await response.text().catch(() => "");
        throw new Error(`API é”™è¯¯ ${response.status}: ${detail.slice(0, 200)}`);
    }

    const reader = response.body.getReader();
    const decoder = new TextDecoder("utf-8");
    let buffer = "";

    while (true) {
        const { done, value } = await reader.read();
        if (done) break;

        buffer += decoder.decode(value, { stream: true });

        let idx = buffer.indexOf("\n");
        while (idx !== -1) {
            const line = buffer.slice(0, idx).trim();
            buffer = buffer.slice(idx + 1);
            idx = buffer.indexOf("\n");

            if (!line.startsWith("data:")) continue;
            const data = line.slice(5).trim();
            if (data === "[DONE]") return;

            try {
                const parsed = JSON.parse(data);
                const delta = parsed.choices?.[0]?.delta;
                if (delta?.content) yield delta.content;
            } catch {
                // skip
            }
        }
    }
}

// â”€â”€â”€ Backend proxy streaming (original SSE format) â”€â”€â”€
async function* backendStream(messages, { sessionId, signal }) {
    const response = await fetch("/api/chat", {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({ messages, sessionId }),
        signal
    });

    if (!response.ok || !response.body) {
        throw new Error("æœåŠ¡å¼‚å¸¸ï¼Œè¯·ç¨åå†è¯•ã€‚");
    }

    const reader = response.body.getReader();
    const decoder = new TextDecoder("utf-8");
    let buffer = "";

    while (true) {
        const { done, value } = await reader.read();
        if (done) break;
        buffer += decoder.decode(value, { stream: true });

        let idx = buffer.indexOf("\n\n");
        while (idx !== -1) {
            const raw = buffer.slice(0, idx);
            buffer = buffer.slice(idx + 2);
            idx = buffer.indexOf("\n\n");

            const lines = raw.split("\n");
            let event = "message";
            let data = "";
            for (const line of lines) {
                if (line.startsWith("event:")) event = line.replace("event:", "").trim();
                else if (line.startsWith("data:")) data += line.replace("data:", "").trim();
            }

            if (event === "chunk") {
                try {
                    const payload = JSON.parse(data);
                    if (payload?.text) yield payload.text;
                } catch { /* skip */ }
            }
            if (event === "error") {
                throw new Error("æœåŠ¡å¼‚å¸¸ï¼Œè¯·ç¨åå†è¯•ã€‚");
            }
        }
    }
}

// â”€â”€â”€ Main hook â”€â”€â”€
export default function useChat({ activeSession, addMessageToSession, settings }) {
    const [isStreaming, setIsStreaming] = useState(false);
    const [streamingReply, setStreamingReply] = useState("");
    const [lastError, setLastError] = useState(null);
    const abortRef = useRef(null);

    const sendMessage = useCallback(async (text) => {
        if (!activeSession || !text.trim()) return;

        const userMessage = { id: crypto.randomUUID(), role: "user", content: text, createdAt: Date.now() };
        addMessageToSession(activeSession.id, userMessage);

        setIsStreaming(true);
        setStreamingReply("");
        setLastError(null);

        const abortController = new AbortController();
        abortRef.current = abortController;

        const allMessages = [...activeSession.messages, userMessage];
        const mode = settings?.apiMode || "direct";

        try {
            let stream;

            if (mode === "mock") {
                stream = mockStream();
            } else if (mode === "direct") {
                stream = openaiStream(allMessages, {
                    apiBase: settings.apiBase,
                    apiKey: settings.apiKey,
                    model: activeSession.model || settings.defaultModel,
                    systemPrompt: settings.systemPrompt,
                    signal: abortController.signal
                });
            } else {
                // backend mode
                stream = backendStream(allMessages, {
                    sessionId: activeSession.id,
                    signal: abortController.signal
                });
            }

            let reply = "";
            for await (const chunk of stream) {
                reply += chunk;
                setStreamingReply(reply);
            }

            addMessageToSession(activeSession.id, {
                id: crypto.randomUUID(),
                role: "assistant",
                content: reply || "æŠ±æ­‰ï¼Œæˆ‘åˆšæ‰æ²¡å¬æ¸…ã€‚",
                createdAt: Date.now()
            });
        } catch (err) {
            const wasCancelled = err?.name === "AbortError";
            setLastError(wasCancelled ? null : (err?.message || "è¯·æ±‚å¤±è´¥"));
            addMessageToSession(activeSession.id, {
                id: crypto.randomUUID(),
                role: "assistant",
                content: wasCancelled ? "ç”Ÿæˆå·²åœæ­¢ã€‚" : `è¯·æ±‚å¤±è´¥ï¼š${err?.message || "æœªçŸ¥é”™è¯¯"}`,
                createdAt: Date.now()
            });
            if (!wasCancelled) setTimeout(() => setLastError(null), 10_000);
        } finally {
            setIsStreaming(false);
            setStreamingReply("");
            abortRef.current = null;
        }
    }, [activeSession, addMessageToSession, settings]);

    const stopStreaming = useCallback(() => {
        if (abortRef.current) abortRef.current.abort();
    }, []);

    return { isStreaming, streamingReply, lastError, sendMessage, stopStreaming };
}
